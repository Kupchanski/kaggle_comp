{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\nImportant: in kernel-only competitions we can't use internet connections. So I use pretrained models from here: https://www.kaggle.com/bminixhofer/pytorch-pretrained-image-models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nimport tqdm\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport copy\nimport cv2\nimport albumentations\nfrom albumentations import torch as AT\nimport random","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(1234)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nsample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n\nold_train = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels_cropped.csv')\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"old_train = old_train[['image','level']]\nold_train = old_train[old_train.level != 0]\nold_train.columns = train.columns\nold_train.diagnosis.value_counts()\n","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"2    5288\n1    2438\n3     872\n4     708\nName: diagnosis, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path columns\ntrain['id_code'] = '../input/aptos2019-blindness-detection/train_images/' + train['id_code'].astype(str) + '.png'\nold_train['id_code'] = '../input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped/' + old_train['id_code'].astype(str) + '.jpeg'","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, old_train], ignore_index=True)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = [i for i in train['id_code'].values]\nimage_test = train_images[4000]\nimage_test","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"'../input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped/1599_left.jpeg'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['diagnosis'].value_counts().plot(kind='bar');\nplt.title('Class counts');","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"We have a slight disbalance in data."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"fig = plt.figure(figsize=(25, 16))\n# display 10 images from each class\nfor class_id in sorted(old_train['level'].unique()):\n    for i, (idx, row) in enumerate(old_train.loc[old_train['level'] == class_id].sample(10).iterrows()):\n        ax = fig.add_subplot(5, 10, class_id * 10 + i + 1, xticks=[], yticks=[])\n        im = Image.open(f'../input/diabetic-retinopathy-resized/resized_train/resized_train/{row[\"image\"]}.jpeg')\n        plt.imshow(im)\n        ax.set_title(f'Label: {level}')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Personally I see little differences between images"},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndef prepare_labels(y):\n    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, le = prepare_labels(train['diagnosis'])","execution_count":29,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/30) ,-4 ,128)\n    \n    return img\n\ndef preprocess_image_old(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/40) ,-4 ,128)\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GlassDataset(Dataset):\n    def __init__(self, df, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), y = None):\n        self.df = df\n        self.datatype = datatype\n        if self.datatype == 'train':\n            #self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.image_files_list = [i for i in df['id_code'].values]\n            self.labels = y\n        else:\n            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.labels = np.zeros((df.shape[0], 5))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files_list[idx]\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=img)\n        image = image['image']\n\n        img_name_short = self.image_files_list[idx].split('.')[0]\n\n        label = self.labels[idx]\n        if self.datatype == 'test':\n            return image, label, img_name\n        else:\n            return image, label","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    albumentations.JpegCompression(80),\n    albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.Normalize(),\n    albumentations.HorizontalFlip(),   \n    AT.ToTensor()\n    ])\n\n\ndataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)\ntest_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\ntr, val = train_test_split(train.diagnosis, stratify=train.diagnosis, test_size=0.15)\ntrain_sampler = SubsetRandomSampler(list(tr.index))\nvalid_sampler = SubsetRandomSampler(list(val.index))\nbatch_size = 16\nnum_workers = 0\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/pretrained-pytorch-models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv = torchvision.models.resnet152()\nmodel_conv.load_state_dict(torch.load('../input/pytorch-pretrained-models/resnet152-b121ed2d.pth'))\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(2048, 5)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"test_model = torchvision.models.resnet18()\ntest_model.load_state_dict(torch.load('../input/pretrained-pytorch-models/resnet18-5c106cde.pth'))\nnum_ftrs = test_model.fc.in_features\ntest_model.fc = nn.Linear(2048, 5)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_conv2 = torchvision.models.resnet50()\nmodel_conv2.load_state_dict(torch.load('../input/pytorch-pretrained-models/resnet50-19c8e357.pth'))\nnum_ftrs2 = model_conv2.fc.in_features\nmodel_conv2.fc = nn.Linear(2048, 5)\"\"\"","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_conv3 = torchvision.models.vgg16()\nmodel_conv3.load_state_dict(torch.load('../input/vgg16/vgg16.pth'))\nmodel_conv3.classifier[6] = nn.Linear(4096,5)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_conv4 = torchvision.models.densenet121()\nmodel_conv4.load_state_dict(torch.load('../input/pytorch-pretrained-image-models/densenet121.pth'))\nmodel_conv4.classifier = nn.Linear(1024, 5)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_conv5 = torchvision.models.inception_v3()\nmodel_conv5.load_state_dict(torch.load('../input/pretrained-pytorch-models/inception_v3_google-1a9a5a14.pth'))\n# Handle the auxilary net\nnum_ftrs = model_conv5.AuxLogits.fc.in_features\nmodel_conv5.AuxLogits.fc = nn.Linear(num_ftrs, 5)\n# Handle the primary net\nnum_ftrs = model_conv5.fc.in_features\nmodel_conv5.fc = nn.Linear(num_ftrs,5)\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1_path = \"../input/aptos1/model_1.pt\"\nmodel2_path = \"../input/aptos1/model_2.pt\"\nmodel3_path = \"../input/aptos1/model_3.pt\"\nmodel4_path = \"../input/aptos1/model_4.pt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs=15, attempt=1):\n    model_conv.to(device)\n    valid_loss_min = np.Inf\n    patience = 5\n    # current number of epochs, where validation loss didn't increase\n    p = 0\n    # whether training should be stopped\n    stop = False\n\n    # number of epochs to train the model\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = []\n        train_auc = []\n\n        for batch_i, (data, target) in enumerate(train_loader):\n\n            data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model_conv(data)\n            loss = criterion(output, target.float())\n            train_loss.append(loss.item())\n\n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # train_auc.append(roc_auc_score(a, b))\n            loss.backward()\n            optimizer.step()\n\n        model_conv.eval()\n        val_loss = []\n        val_auc = []\n        for batch_i, (data, target) in enumerate(valid_loader):\n            data, target = data.cuda(), target.cuda()\n            output = model_conv(data)\n\n            loss = criterion(output, target.float())\n\n            val_loss.append(loss.item()) \n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # val_auc.append(roc_auc_score(a, b))\n\n        # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n\n        valid_loss = np.mean(val_loss)\n        scheduler.step(valid_loss)\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model_conv.state_dict(), 'model_{}.pt'.format(attempt))\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n    return model_conv","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.99)\n#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\nscheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, )","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15, attempt=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_resnet50 = train_model(model_conv2, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=1, attempt=2)\"\"\"","execution_count":40,"outputs":[{"output_type":"stream","text":"Wed Aug 21 13:13:13 2019 Epoch: 1\nEpoch 1, train loss: 0.7765, valid loss: 0.7765.\nValidation loss decreased (inf --> 0.776478).  Saving model ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_densenet121 = train_model(model_conv4, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15, attempt=4)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"model_vgg16 = train_model(model_conv3, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15, attempt=3)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_inception = train_model(model_conv5, train_loader_inc, valid_loader_inc, criterion = criterion, \n#                              optimizer = optimizer, n_epochs=15, attempt=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, test_loader):\n    sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n    model.eval()\n    for (data, target, name) in test_loader:\n        data = data.cuda()\n        output = model(data)\n        output = output.cpu().detach().numpy()\n        for i, (e, n) in enumerate(list(zip(output, name))):\n            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n    print( \"done\")\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_resnet101_1 = test_model(model_resnet101, test_loader)\ntest_resnet101_2 = test_model(model_resnet101, test_loader)\ntest_resnet101_3 = test_model(model_resnet101, test_loader)\ntest_resnet101_4 = test_model(model_resnet101, test_loader)\ntest_resnet101_5 = test_model(model_resnet101, test_loader)\ntest_resnet101_6 = test_model(model_resnet101, test_loader)\ntest_resnet101_7 = test_model(model_resnet101, test_loader)\ntest_resnet101_8 = test_model(model_resnet101, test_loader)\ntest_resnet101_9 = test_model(model_resnet101, test_loader)\ntest_resnet101_10 = test_model(model_resnet101, test_loader)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = (test_resnet101_1.diagnosis + test_resnet101_2.diagnosis + test_resnet101_3.diagnosis + test_resnet101_4.diagnosis + test_resnet101_5.diagnosis \n              + test_resnet101_6.diagnosis + test_resnet101_7.diagnosis  \n               + test_resnet101_8.diagnosis + test_resnet101_9.diagnosis + test_resnet101_10.diagnosis) / 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = [0.5, 1.5, 2.5, 3.5]\n\nfor i, pred in enumerate(test_preds):\n    if pred < coef[0]:\n        test_preds[i] = 0\n    elif pred >= coef[0] and pred < coef[1]:\n        test_preds[i] = 1\n    elif pred >= coef[1] and pred < coef[2]:\n        test_preds[i] = 2\n    elif pred >= coef[2] and pred < coef[3]:\n        test_preds[i] = 3\n    else:\n        test_preds[i] = 4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\nsample.diagnosis = test_preds.astype(int)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_resnet50 = test_model(model_resnet50, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_densenet121 = test_model(model_densenet121, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_vgg16 = test_model(model_vgg16, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_inception = test_model(model_inception, test_loader_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n\nmodel_conv.eval()\nfor (data, target, name) in test_loader:\n    data = data.cuda()\n    output = model_conv(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n        \nsub.to_csv('submission.csv', index=False)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub['diagnosis'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}