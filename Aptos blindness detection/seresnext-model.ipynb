{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Important: in kernel-only competitions we can't use internet connections. So I use pretrained models from here: https://www.kaggle.com/bminixhofer/pytorch-pretrained-image-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pretrained-models/pretrained-models/pretrained-models.pytorch-master\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (1.2.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (0.4.0a0+6b959ee)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (2.3.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (4.32.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->pretrainedmodels==0.7.4) (1.17.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.12.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->pretrainedmodels==0.7.4) (5.4.1)\r\n",
      "Building wheels for collected packages: pretrainedmodels\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=62050 sha256=9c44bdf7f7784a50a03dbcea64f64c9e476035decf102f36af208b497e1fa3a1\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/e3/b8/7a/de7530d7995e405dccf81bf0c8c573c271cb6d012d330300f1\r\n",
      "Successfully built pretrainedmodels\r\n",
      "Installing collected packages: pretrainedmodels\r\n",
      "Successfully installed pretrainedmodels-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"../input/pretrained-models/pretrained-models/pretrained-models.pytorch-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrainedmodels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-67b4e602e797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrainedmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrainedmodels' is not defined"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time \n",
    "import tqdm\n",
    "from PIL import Image\n",
    "train_on_gpu = True\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import copy\n",
    "import cv2\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "import random\n",
    "\n",
    "\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "sample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "\n",
    "old_train = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels_cropped.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"old_train = old_train[['image','level']]\\nold_train.columns = train.columns\\nold_train.diagnosis.value_counts()\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"old_train = old_train[['image','level']]\n",
    "old_train.columns = train.columns\n",
    "old_train.diagnosis.value_counts()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# path columns\\ntrain['id_code'] = '../input/aptos2019-blindness-detection/train_images/' + train['id_code'].astype(str) + '.png'\\nold_train['id_code'] = '../input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped/' + old_train['id_code'].astype(str) + '.jpeg'\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# path columns\n",
    "train['id_code'] = '../input/aptos2019-blindness-detection/train_images/' + train['id_code'].astype(str) + '.png'\n",
    "old_train['id_code'] = '../input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped/' + old_train['id_code'].astype(str) + '.jpeg'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train = pd.concat([train, old_train], ignore_index=True)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train = pd.concat([train, old_train], ignore_index=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_images = [i for i in train['id_code'].values]\\nimage_test = train_images[4000]\\nimage_test\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_images = [i for i in train['id_code'].values]\n",
    "image_test = train_images[4000]\n",
    "image_test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['diagnosis'].value_counts().plot(kind='bar');\n",
    "plt.title('Class counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a slight disbalance in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally I see little differences between images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "def prepare_labels(y):\n",
    "    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y, le = prepare_labels(train['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/30) ,-4 ,128)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_image_old(image_path, desired_size=224):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/40) ,-4 ,128)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlassDataset(Dataset):\n",
    "    def __init__(self, df, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), y = None):\n",
    "        self.df = df\n",
    "        self.datatype = datatype\n",
    "        if self.datatype == 'train':\n",
    "            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n",
    "            self.labels = y\n",
    "        else:\n",
    "            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n",
    "            self.labels = np.zeros((df.shape[0], 5))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files_list[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image=img)\n",
    "        image = image['image']\n",
    "\n",
    "        img_name_short = self.image_files_list[idx].split('.')[0]\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        if self.datatype == 'test':\n",
    "            return image, label, img_name\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.RandomBrightness(),\n",
    "    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n",
    "    albumentations.JpegCompression(80),\n",
    "    albumentations.HueSaturationValue(),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "data_transforms_test = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.Normalize(),\n",
    "    albumentations.HorizontalFlip(),    \n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "dataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)\n",
    "test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n",
    "tr, val = train_test_split(train.diagnosis, stratify=train.diagnosis, test_size=0.1)\n",
    "train_sampler = SubsetRandomSampler(list(tr.index))\n",
    "valid_sampler = SubsetRandomSampler(list(val.index))\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['se_resnext50_32x4d-a260b3a4.pth']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../input/seresnext50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = pretrainedmodels.__dict__['se_resnext50_32x4d']( pretrained=None)\n",
    "model_conv.load_state_dict(torch.load(\"../input/seresnext50/se_resnext50_32x4d-a260b3a4.pth\"))\n",
    "model_conv.last_linear = nn.Linear(2048, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs=15, attempt=1):\n",
    "    model_conv.to(device)\n",
    "    valid_loss_min = np.Inf\n",
    "    patience = 5\n",
    "    # current number of epochs, where validation loss didn't increase\n",
    "    p = 0\n",
    "    # whether training should be stopped\n",
    "    stop = False\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "        train_loss = []\n",
    "        train_auc = []\n",
    "\n",
    "        for batch_i, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_conv(data)\n",
    "            loss = criterion(output, target.float())\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output[:,-1].detach().cpu().numpy()\n",
    "            # train_auc.append(roc_auc_score(a, b))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model_conv.eval()\n",
    "        val_loss = []\n",
    "        val_auc = []\n",
    "        for batch_i, (data, target) in enumerate(valid_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model_conv(data)\n",
    "\n",
    "            loss = criterion(output, target.float())\n",
    "\n",
    "            val_loss.append(loss.item()) \n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output[:,-1].detach().cpu().numpy()\n",
    "            # val_auc.append(roc_auc_score(a, b))\n",
    "\n",
    "        # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n",
    "        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n",
    "\n",
    "        valid_loss = np.mean(val_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model_conv.state_dict(), 'model_{}.pt'.format(attempt))\n",
    "            valid_loss_min = valid_loss\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_loss > valid_loss_min:\n",
    "            p += 1\n",
    "            print(f'{p} epochs of increasing val loss')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break        \n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model_conv.last_linear.parameters(), lr=0.001, momentum=0.99)\n",
    "#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 21 13:56:17 2019 Epoch: 1\n",
      "Epoch 1, train loss: 0.4091, valid loss: 0.3201.\n",
      "Validation loss decreased (inf --> 0.320077).  Saving model ...\n",
      "Wed Aug 21 14:01:10 2019 Epoch: 2\n",
      "Epoch 2, train loss: 0.2695, valid loss: 0.2711.\n",
      "Validation loss decreased (0.320077 --> 0.271050).  Saving model ...\n",
      "Wed Aug 21 14:05:48 2019 Epoch: 3\n",
      "Epoch 3, train loss: 0.2427, valid loss: 0.2502.\n",
      "Validation loss decreased (0.271050 --> 0.250186).  Saving model ...\n",
      "Wed Aug 21 14:10:26 2019 Epoch: 4\n",
      "Epoch 4, train loss: 0.2341, valid loss: 0.2482.\n",
      "Validation loss decreased (0.250186 --> 0.248177).  Saving model ...\n",
      "Wed Aug 21 14:15:05 2019 Epoch: 5\n",
      "Epoch 5, train loss: 0.2282, valid loss: 0.2550.\n",
      "1 epochs of increasing val loss\n",
      "Wed Aug 21 14:19:44 2019 Epoch: 6\n",
      "Epoch 6, train loss: 0.2240, valid loss: 0.2498.\n",
      "2 epochs of increasing val loss\n",
      "Wed Aug 21 14:24:24 2019 Epoch: 7\n",
      "Epoch 7, train loss: 0.2219, valid loss: 0.2499.\n",
      "3 epochs of increasing val loss\n",
      "Wed Aug 21 14:29:19 2019 Epoch: 8\n",
      "Epoch 8, train loss: 0.2234, valid loss: 0.2574.\n",
      "4 epochs of increasing val loss\n",
      "Wed Aug 21 14:33:59 2019 Epoch: 9\n",
      "Epoch 9, train loss: 0.2153, valid loss: 0.2373.\n",
      "Validation loss decreased (0.248177 --> 0.237292).  Saving model ...\n",
      "Wed Aug 21 14:38:39 2019 Epoch: 10\n",
      "Epoch 10, train loss: 0.2153, valid loss: 0.2404.\n",
      "1 epochs of increasing val loss\n",
      "Wed Aug 21 14:43:24 2019 Epoch: 11\n",
      "Epoch 11, train loss: 0.2127, valid loss: 0.2436.\n",
      "2 epochs of increasing val loss\n",
      "Wed Aug 21 14:48:03 2019 Epoch: 12\n",
      "Epoch 12, train loss: 0.2094, valid loss: 0.2442.\n",
      "3 epochs of increasing val loss\n",
      "Wed Aug 21 14:52:48 2019 Epoch: 13\n",
      "Epoch 13, train loss: 0.2074, valid loss: 0.2360.\n",
      "Validation loss decreased (0.237292 --> 0.236009).  Saving model ...\n",
      "Wed Aug 21 14:57:27 2019 Epoch: 14\n",
      "Epoch 14, train loss: 0.2076, valid loss: 0.2532.\n",
      "1 epochs of increasing val loss\n",
      "Wed Aug 21 15:02:06 2019 Epoch: 15\n",
      "Epoch 15, train loss: 0.2079, valid loss: 0.2380.\n",
      "2 epochs of increasing val loss\n"
     ]
    }
   ],
   "source": [
    "model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n",
    "                              optimizer = optimizer, n_epochs=15, attempt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "    model.eval()\n",
    "    for (data, target, name) in test_loader:\n",
    "        data = data.cuda()\n",
    "        output = model(data)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        for i, (e, n) in enumerate(list(zip(output, name))):\n",
    "            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n",
    "    print( \"done\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "test_resnet101_1 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_2 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_3 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_4 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_5 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_6 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_7 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_8 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_9 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_10 = test_model(model_resnet101, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = (test_resnet101_1.diagnosis + test_resnet101_2.diagnosis + test_resnet101_3.diagnosis + test_resnet101_4.diagnosis + test_resnet101_5.diagnosis \n",
    "              + test_resnet101_6.diagnosis + test_resnet101_7.diagnosis  \n",
    "               + test_resnet101_8.diagnosis + test_resnet101_9.diagnosis + test_resnet101_10.diagnosis) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = test_preds.astype(int)\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
