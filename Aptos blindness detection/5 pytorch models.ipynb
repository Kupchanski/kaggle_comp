{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I work with data from APTOS 2019 Blindness Detection competition\n\n![](https://nei.nih.gov/sites/default/files/health-images/macula_dr.gif)\n\nOur task is to detect blindeness. This problem feels quite important for me - I'm not blind or near it, but I have a bad eyesight, so I know that problems with eye are serious.\n\nIn this kernel I'll do a basic EDA and train a baseline pytorch model.\n\nImportant: in kernel-only competitions we can't use internet connections. So I use pretrained models from here: https://www.kaggle.com/bminixhofer/pytorch-pretrained-image-models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nimport tqdm\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport copy\nimport cv2\nimport albumentations\nfrom albumentations import torch as AT","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nsample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n\nold_train = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"        id_code  diagnosis\n0  000c1434d8d7          2\n1  001639a390f0          4\n2  0024cdab0c1e          1\n3  002c21358ce6          0\n4  005b95c28852          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c1434d8d7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001639a390f0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0024cdab0c1e</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002c21358ce6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>005b95c28852</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['diagnosis'].value_counts().plot(kind='bar');\nplt.title('Class counts');","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"We have a slight disbalance in data."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"fig = plt.figure(figsize=(25, 16))\n# display 10 images from each class\nfor class_id in sorted(old_train['level'].unique()):\n    for i, (idx, row) in enumerate(old_train.loc[old_train['level'] == class_id].sample(10).iterrows()):\n        ax = fig.add_subplot(5, 10, class_id * 10 + i + 1, xticks=[], yticks=[])\n        im = Image.open(f'../input/diabetic-retinopathy-resized/resized_train/resized_train/{row[\"image\"]}.jpeg')\n        plt.imshow(im)\n        ax.set_title(f'Label: {level}')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Personally I see little differences between images"},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndef prepare_labels(y):\n    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, le = prepare_labels(train['diagnosis'])","execution_count":7,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GlassDataset(Dataset):\n    def __init__(self, df, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), y = None):\n        self.df = df\n        self.datatype = datatype\n        if self.datatype == 'train':\n            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.labels = y\n        else:\n            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.labels = np.zeros((df.shape[0], 5))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files_list[idx]\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=img)\n        image = image['image']\n\n        img_name_short = self.image_files_list[idx].split('.')[0]\n\n        label = self.labels[idx]\n        if self.datatype == 'test':\n            return image, label, img_name\n        else:\n            return image, label","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)","execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_transforms' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-0d2440f114c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlassDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data_transforms' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    albumentations.JpegCompression(80),\n    albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_inc = albumentations.Compose([\n    albumentations.Resize(229, 229),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    albumentations.JpegCompression(80),\n    albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test_inc = albumentations.Compose([\n    albumentations.Resize(229, 229),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\n\ndataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)\ntest_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\ndataset_inc = GlassDataset(df=train, datatype='train', transform=data_transforms_inc, y=y)\ntest_set_inc = GlassDataset(df=test, datatype='test', transform=data_transforms_test_inc)\ntr, val = train_test_split(train.diagnosis, stratify=train.diagnosis, test_size=0.1)\ntrain_sampler = SubsetRandomSampler(list(tr.index))\nvalid_sampler = SubsetRandomSampler(list(val.index))\nbatch_size = 16\nnum_workers = 0\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n\ntrain_loader_inc = torch.utils.data.DataLoader(dataset_inc, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader_inc = torch.utils.data.DataLoader(dataset_inc, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\ntest_loader_inc = torch.utils.data.DataLoader(test_set_inc, batch_size=batch_size, num_workers=num_workers)\n","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv = torchvision.models.resnet101()\nmodel_conv.load_state_dict(torch.load('../input/pytorch-pretrained-models/resnet101-5d3b4d8f.pth'))\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(2048, 5)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv2 = torchvision.models.resnet50()\nmodel_conv2.load_state_dict(torch.load('../input/pytorch-pretrained-models/resnet50-19c8e357.pth'))\nnum_ftrs2 = model_conv2.fc.in_features\nmodel_conv2.fc = nn.Linear(2048, 5)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/pretrained-pytorch-models\")","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"['resnet50-19c8e357.pth',\n 'squeezenet1_1-f364aa15.pth',\n 'inception_v3_google-1a9a5a14.pth',\n 'squeezenet1_0-a815701f.pth',\n 'resnet18-5c106cde.pth',\n 'densenet161-17b70270.pth']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv3 = torchvision.models.vgg16()\nmodel_conv3.load_state_dict(torch.load('../input/vgg16/vgg16.pth'))\nmodel_conv3.classifier[6] = nn.Linear(4096,5)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv4 = torchvision.models.densenet121()\nmodel_conv4.load_state_dict(torch.load('../input/pytorch-pretrained-image-models/densenet121.pth'))\nmodel_conv4.classifier = nn.Linear(1024, 5)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv5 = torchvision.models.inception_v3()\nmodel_conv5.load_state_dict(torch.load('../input/pretrained-pytorch-models/inception_v3_google-1a9a5a14.pth'))\nmodel_conv5.AuxLogits.fc = nn.Linear(768, 5)\nmodel_conv5.fc = nn.Linear(2048, 5)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs=15, attempt=1):\n    valid_loss_min = np.Inf\n    patience = 5\n    # current number of epochs, where validation loss didn't increase\n    p = 0\n    # whether training should be stopped\n    stop = False\n\n    # number of epochs to train the model\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = []\n        train_auc = []\n\n        for batch_i, (data, target) in enumerate(train_loader):\n\n            data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model_conv(data)\n            loss = criterion(output, target.float())\n            train_loss.append(loss.item())\n\n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # train_auc.append(roc_auc_score(a, b))\n            loss.backward()\n            optimizer.step()\n\n        model_conv.eval()\n        val_loss = []\n        val_auc = []\n        for batch_i, (data, target) in enumerate(valid_loader):\n            data, target = data.cuda(), target.cuda()\n            output = model_conv(data)\n\n            loss = criterion(output, target.float())\n\n            val_loss.append(loss.item()) \n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # val_auc.append(roc_auc_score(a, b))\n\n        # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n\n        valid_loss = np.mean(val_loss)\n        scheduler.step(valid_loss)\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model_conv.state_dict(), 'model_{}.pt'.format(attempt))\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n    return model_conv, model_conv","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.99)\n#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\nscheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, )","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_loss_min = np.Inf\npatience = 5\n# current number of epochs, where validation loss didn't increase\np = 0\n# whether training should be stopped\nstop = False\n\n# number of epochs to train the model\nn_epochs = 20\nfor epoch in range(1, n_epochs+1):\n    print(time.ctime(), 'Epoch:', epoch)\n\n    train_loss = []\n    train_auc = []\n\n    for batch_i, (data, target) in enumerate(train_loader):\n\n        data, target = data.cuda(), target.cuda()\n\n        optimizer.zero_grad()\n        output = model_conv(data)\n        loss = criterion(output, target.float())\n        train_loss.append(loss.item())\n        \n        a = target.data.cpu().numpy()\n        b = output[:,-1].detach().cpu().numpy()\n        # train_auc.append(roc_auc_score(a, b))\n        loss.backward()\n        optimizer.step()\n    \n    model_conv.eval()\n    val_loss = []\n    val_auc = []\n    for batch_i, (data, target) in enumerate(valid_loader):\n        data, target = data.cuda(), target.cuda()\n        output = model_conv(data)\n\n        loss = criterion(output, target.float())\n\n        val_loss.append(loss.item()) \n        a = target.data.cpu().numpy()\n        b = output[:,-1].detach().cpu().numpy()\n        # val_auc.append(roc_auc_score(a, b))\n\n    # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n    print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n    \n    valid_loss = np.mean(val_loss)\n    scheduler.step(valid_loss)\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model_conv.state_dict(), 'model_old_2.pt')\n        valid_loss_min = valid_loss\n        p = 0\n\n    # check if validation loss didn't improve\n    if valid_loss > valid_loss_min:\n        p += 1\n        print(f'{p} epochs of increasing val loss')\n        if p > patience:\n            print('Stopping training')\n            stop = True\n            break        \n            \n    if stop:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloaders = {'train':train_loader, \"val\":valid_loader }\ndataloaders_inc = {'train':train_loader_inc, \"val\":valid_loader_inc }","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15)","execution_count":57,"outputs":[{"output_type":"stream","text":"Sun Aug 18 10:56:56 2019 Epoch: 1\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.84 GiB already allocated; 9.88 MiB free; 376.55 MiB cached)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-c2f230dd6e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n\u001b[0;32m----> 2\u001b[0;31m                               optimizer = optimizer, n_epochs=15)\n\u001b[0m","\u001b[0;32m<ipython-input-56-9e517d4e9824>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs, attempt)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.84 GiB already allocated; 9.88 MiB free; 376.55 MiB cached)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnet50 = train_model(model_conv2, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_densenet121 = train_model(model_conv4, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vgg16 = train_model(model_conv3, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_inception = train_model(model_conv5, train_loader_inc, valid_loader_inc, criterion = criterion, \n                              optimizer = optimizer, n_epochs=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, test_loader):\n    sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n    model.eval()\n    for (data, target, name) in test_loader:\n        data = data.cuda()\n        output = model(data)\n        output = output.cpu().detach().numpy()\n        for i, (e, n) in enumerate(list(zip(output, name))):\n            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n    return sub","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_resnet101 = test_model(model_resnet101, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_resnet50 = test_model(model_resnet50, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_densenet121 = test_model(model_densenet121, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vgg16 = test_model(model_vgg16, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_inception = test_model(model_inception, test_loader_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n\nmodel_conv.eval()\nfor (data, target, name) in test_loader:\n    data = data.cuda()\n    output = model_conv(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n        \nsub.to_csv('submission.csv', index=False)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['diagnosis'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}