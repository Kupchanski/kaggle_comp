{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\nImportant: in kernel-only competitions we can't use internet connections. So I use pretrained models from here: https://www.kaggle.com/bminixhofer/pytorch-pretrained-image-models"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install \"../input/pretrained-models/pretrained-models/pretrained-models.pytorch-master\"","execution_count":1,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/pretrained-models/pretrained-models/pretrained-models.pytorch-master\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (1.2.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (0.4.0a0+6b959ee)\nRequirement already satisfied: munch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (2.3.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4) (4.32.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->pretrainedmodels==0.7.4) (1.17.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.12.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->pretrainedmodels==0.7.4) (5.4.1)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=62050 sha256=036f62e8ef8c96722765543f95db507e8cadc61b164ade81707b4c46d85f5aff\n  Stored in directory: /tmp/.cache/pip/wheels/e3/b8/7a/de7530d7995e405dccf81bf0c8c573c271cb6d012d330300f1\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\n^C\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nimport tqdm\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport copy\nimport cv2\nimport albumentations\nfrom albumentations import torch as AT\nimport random\n\n\nimport pretrainedmodels","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(1234)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nsample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nold_train = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels_cropped.csv')\n\n\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"old_train = old_train[['image','level']]\nold_train.columns = train.columns\nold_train.diagnosis.value_counts()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# path columns\ntrain['id_code'] = '../input/aptos2019-blindness-detection/train_images/' + train['id_code'].astype(str) + '.png'\nold_train['id_code'] = '../input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped/' + old_train['id_code'].astype(str) + '.jpeg'\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"train = pd.concat([train, old_train], ignore_index=True)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"train_images = [i for i in train['id_code'].values]\nimage_test = train_images[4000]\nimage_test\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['diagnosis'].value_counts().plot(kind='bar');\nplt.title('Class counts');","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYEAAAEFCAYAAAAFeFvqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFF5JREFUeJzt3X+w3XV95/HnaxOhZUWkzZVCwjWIwRa0ppJBnK4OViuBuoK7a0vqAnXdiXRgVke3U9TOwrplx2lLdZ1FXKxZYNcFUURYiz8irtLOihIwht8SMMglIURwAYulJr73j/O95vTm3pube27uSfw8HzNn7jnv7+f7/b7PgdzX+X6+33NuqgpJUpv+ybAbkCQNjyEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0A/F5JcmOR/DrsPaX9jCGi/keT3k6xL8qMkW5J8Ick/G3Zf8yVJJXnxsPvQzxdDQPuFJO8GPgz8Z+AwYBT4KHDaMPuS9neGgPZ5SQ4BPgCcW1Wfraq/q6qfVNX/rqo/mmKdTyd5NMmTSW5OclzfslOT3J3k6SSPJPn3XX1Rks8n+X9JnkjyN0km/TeS5Lgka7txW5O8r6sfmOTDSTZ3tw8nObBb9gdJ/nbCdn727j7J5UkuSfLXXW/fTHJ0t+zmbpXvdEdCv7cn/UpT8X8Y7Q9eBfwCcN0erPMFYBnwAuB24JN9yz4BvKOqDgZeCny1q78HGANG6B1tvA/Y5XtVkhwMfAX4InAE8GLgpm7x+4ETgeXAy4ETgD/Zg75XAf8ROBTYCFwEUFWv6Za/vKqeW1Wfmmm/0nQMAe0Pfhn4QVVtn+kKVbWmqp6uqmeBC4GXd0cUAD8Bjk3yvKr6YVXd3lc/HHhhd6TxNzX5l2u9EXi0qi6uqr/v9vPNbtlbgQ9U1WNVtY3eL/Qz9+C5fraqvtU910/SC5OpzLRfaUqGgPYHjwOLkiycyeAkC5J8MMkDSZ4CNnWLFnU//yVwKvBQkq8neVVX/3N6776/nOTBJOdPsYsjgQemWHYE8FDf44e62kw92nf/GeC504ydab/SlAwB7Q++Afw9cPoMx/8+vRPGrwcOAZZ29QBU1a1VdRq9qaLPAdd09aer6j1V9SLgnwPvTvK6Sbb/MHD0FPveDLyw7/FoVwP4O+Cg8QVJfmWGz2dSe9CvNCVDQPu8qnoS+A/AJUlOT3JQkuckOSXJn02yysHAs/SOIA6id0URAEkOSPLWJIdU1U+Ap4Ad3bI3JnlxkvTVd0yy/c8Dv5LkXd2J4IOTvLJbdhXwJ0lGkizq+h7//MJ3gOOSLE/yC/SmqfbEVuBFfc9lpv1KUzIEtF+oqr8E3k3vJOs2eu/Gz6P3Tn6iK+lNwzwC3A3cMmH5mcCmbqroHOBfd/Vl9E74/oje0cdHq+prk/TyNPDb9N59PwrcD7y2W/ynwDpgA3AHvZPSf9qt9116Vzl9pVvnH10pNAMXAld0VwP97kz7laYTzyNJUrs8EpCkhhkCktQwQ0CSGmYISFLDDAFJatiMPoE5TIsWLaqlS5cOuw1J2m/cdtttP6iqkZmM3edDYOnSpaxbt27YbUjSfiPJQ7sf1eN0kCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlh+/yHxQa19Py/HnYLAGz64O8MuwVJ2oVHApLUMENAkhq22xBIsibJY0nu7Kt9Ksn67rYpyfquvjTJj/uWfaxvneOT3JFkY5KPdH8cW5I0RDM5J3A58F/p/fFuAKrq98bvJ7kYeLJv/ANVtXyS7VwKrKb3R79vBFYCX9jzliVJc2W3RwJVdTPwxGTLunfzvwtcNd02khwOPK+qvlG9v2x/JXD6nrcrSZpLg54TeDWwtaru76sdleTbSb6e5NVdbTEw1jdmrKtJkoZo0EtEV/GPjwK2AKNV9XiS44HPJTkOmGz+v6baaJLV9KaOGB0dHbBFSdJUZn0kkGQh8C+AT43XqurZqnq8u38b8ABwDL13/kv6Vl8CbJ5q21V1WVWtqKoVIyMz+uM4kqRZGGQ66PXAvVX1s2meJCNJFnT3XwQsAx6sqi3A00lO7M4jnAVcP8C+JUlzYCaXiF4FfAN4SZKxJG/vFp3BrieEXwNsSPId4DPAOVU1flL5D4G/AjbSO0LwyiBJGrLdnhOoqlVT1P9gktq1wLVTjF8HvHQP+5Mk7UV+YliSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ3bbQgkWZPksSR39tUuTPJIkvXd7dS+Ze9NsjHJfUlO7quv7Gobk5w/909FkrSnZnIkcDmwcpL6h6pqeXe7ESDJscAZwHHdOh9NsiDJAuAS4BTgWGBVN1aSNEQLdzegqm5OsnSG2zsNuLqqngW+l2QjcEK3bGNVPQiQ5Opu7N173LEkac4Mck7gvCQbuumiQ7vaYuDhvjFjXW2q+qSSrE6yLsm6bdu2DdCiJGk6sw2BS4GjgeXAFuDirp5JxtY09UlV1WVVtaKqVoyMjMyyRUnS7ux2OmgyVbV1/H6SjwOf7x6OAUf2DV0CbO7uT1WXJA3JrI4Ekhze9/DNwPiVQzcAZyQ5MMlRwDLgW8CtwLIkRyU5gN7J4xtm37YkaS7s9kggyVXAScCiJGPABcBJSZbTm9LZBLwDoKruSnINvRO+24Fzq2pHt53zgC8BC4A1VXXXnD8bSdIemcnVQasmKX9imvEXARdNUr8RuHGPupMk7VV+YliSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlq2G5DIMmaJI8lubOv9udJ7k2yIcl1SZ7f1Zcm+XGS9d3tY33rHJ/kjiQbk3wkSfbOU5IkzdRMjgQuB1ZOqK0FXlpVvw58F3hv37IHqmp5dzunr34psBpY1t0mblOSNM92GwJVdTPwxITal6tqe/fwFmDJdNtIcjjwvKr6RlUVcCVw+uxaliTNlbk4J/BvgC/0PT4qybeTfD3Jq7vaYmCsb8xYV5MkDdHCQVZO8n5gO/DJrrQFGK2qx5McD3wuyXHAZPP/Nc12V9ObOmJ0dHSQFiVJ05j1kUCSs4E3Am/tpnioqmer6vHu/m3AA8Ax9N75908ZLQE2T7XtqrqsqlZU1YqRkZHZtihJ2o1ZhUCSlcAfA2+qqmf66iNJFnT3X0TvBPCDVbUFeDrJid1VQWcB1w/cvSRpILudDkpyFXASsCjJGHABvauBDgTWdld63tJdCfQa4ANJtgM7gHOqavyk8h/Su9LoF+mdQ+g/jyBJGoLdhkBVrZqk/Ikpxl4LXDvFsnXAS/eoO0nSXuUnhiWpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1LAZhUCSNUkeS3JnX+2XkqxNcn/389CuniQfSbIxyYYkr+hb5+xu/P1Jzp77pyNJ2hMzPRK4HFg5oXY+cFNVLQNu6h4DnAIs626rgUuhFxrABcArgROAC8aDQ5I0HDMKgaq6GXhiQvk04Iru/hXA6X31K6vnFuD5SQ4HTgbWVtUTVfVDYC27BoskaR4Nck7gsKraAtD9fEFXXww83DdurKtNVZckDcnCvbDNTFKraeq7biBZTW8qidHR0bnrrHUXHjLsDnoufHLYHUjqDHIksLWb5qH7+VhXHwOO7Bu3BNg8TX0XVXVZVa2oqhUjIyMDtChJms4gIXADMH6Fz9nA9X31s7qrhE4Enuymi74EvCHJod0J4Td0NUnSkMxoOijJVcBJwKIkY/Su8vkgcE2StwPfB97SDb8ROBXYCDwDvA2gqp5I8p+AW7txH6iqiSebJUnzaEYhUFWrplj0uknGFnDuFNtZA6yZcXeSpL3KTwxLUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhsw6BJC9Jsr7v9lSSdyW5MMkjffVT+9Z5b5KNSe5LcvLcPAVJ0mwtnO2KVXUfsBwgyQLgEeA64G3Ah6rqL/rHJzkWOAM4DjgC+EqSY6pqx2x7kCQNZq6mg14HPFBVD00z5jTg6qp6tqq+B2wETpij/UuSZmGuQuAM4Kq+x+cl2ZBkTZJDu9pi4OG+MWNdbRdJVidZl2Tdtm3b5qhFSdJEA4dAkgOANwGf7kqXAkfTmyraAlw8PnSS1WuybVbVZVW1oqpWjIyMDNqiJGkKc3EkcApwe1VtBaiqrVW1o6p+CnycnVM+Y8CRfestATbPwf4lSbM0FyGwir6poCSH9y17M3Bnd/8G4IwkByY5ClgGfGsO9i9JmqVZXx0EkOQg4LeBd/SV/yzJcnpTPZvGl1XVXUmuAe4GtgPnemWQJA3XQCFQVc8AvzyhduY04y8CLhpkn5KkueMnhiWpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGDRwCSTYluSPJ+iTrutovJVmb5P7u56FdPUk+kmRjkg1JXjHo/iVJszdXRwKvrarlVbWie3w+cFNVLQNu6h4DnAIs626rgUvnaP+SpFnYW9NBpwFXdPevAE7vq19ZPbcAz09y+F7qQZK0G3MRAgV8OcltSVZ3tcOqagtA9/MFXX0x8HDfumNdTZI0BAvnYBu/WVWbk7wAWJvk3mnGZpJa7TKoFyarAUZHR+egRUnSZAY+Eqiqzd3Px4DrgBOArePTPN3Px7rhY8CRfasvATZPss3LqmpFVa0YGRkZtEVJ0hQGCoEk/zTJweP3gTcAdwI3AGd3w84Gru/u3wCc1V0ldCLw5Pi0kSRp/g06HXQYcF2S8W39r6r6YpJbgWuSvB34PvCWbvyNwKnARuAZ4G0D7l+SNICBQqCqHgRePkn9ceB1k9QLOHeQfUqS5o6fGJakhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUsLn4Kmlpv/OyK1427BYAuOPsO4bdghrnkYAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYX5YTGrcPb/6a8NuAYBfu/eeYbfQpFkfCSQ5Msn/SXJPkruSvLOrX5jkkSTru9upfeu8N8nGJPclOXkunoAkafYGORLYDrynqm5PcjBwW5K13bIPVdVf9A9OcixwBnAccATwlSTHVNWOAXqQJA1g1kcCVbWlqm7v7j8N3AMsnmaV04Crq+rZqvoesBE4Ybb7lyQNbk5ODCdZCvwG8M2udF6SDUnWJDm0qy0GHu5bbYzpQ0OStJcNHAJJngtcC7yrqp4CLgWOBpYDW4CLx4dOsnpNsc3VSdYlWbdt27ZBW5QkTWGgEEjyHHoB8Mmq+ixAVW2tqh1V9VPg4+yc8hkDjuxbfQmwebLtVtVlVbWiqlaMjIwM0qIkaRqDXB0U4BPAPVX1l331w/uGvRm4s7t/A3BGkgOTHAUsA7412/1LkgY3yNVBvwmcCdyRZH1Xex+wKslyelM9m4B3AFTVXUmuAe6md2XRuV4ZJEnDNesQqKq/ZfJ5/hunWeci4KLZ7lOSNLf82ghJaphfGyFJnUvO+eqwWwDg3I/91rztyyMBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNm/cQSLIyyX1JNiY5f773L0naaV5DIMkC4BLgFOBYYFWSY+ezB0nSTvN9JHACsLGqHqyqfwCuBk6b5x4kSZ1U1fztLPlXwMqq+rfd4zOBV1bVeRPGrQZWdw9fAtw3b01ObhHwgyH3sK/wtdjJ12InX4ud9oXX4oVVNTKTgQv3dicTZJLaLilUVZcBl+39dmYmybqqWjHsPvYFvhY7+Vrs5Gux0/72Wsz3dNAYcGTf4yXA5nnuQZLUme8QuBVYluSoJAcAZwA3zHMPkqTOvE4HVdX2JOcBXwIWAGuq6q757GGW9pmpqX2Ar8VOvhY7+VrstF+9FvN6YliStG/xE8OS1DBDQJIaZghIUsPm+3MC+4Ukv0rvk8yL6X2OYTNwQ1XdM9TGhqB7LRYD36yqH/XVV1bVF4fXmfYVSa6sqrOG3cewJDkBqKq6tfsanJXAvVV145BbmxFPDE+Q5I+BVfS+0mKsKy+hdznr1VX1wWH1Nt+S/DvgXOAeYDnwzqq6vlt2e1W9Ypj97SuSvK2q/vuw+5gPSSZe0h3gtcBXAarqTfPe1BAluYDed6EtBNYCrwS+Brwe+FJVXTS87mbGEJggyXeB46rqJxPqBwB3VdWy4XQ2/5LcAbyqqn6UZCnwGeB/VNV/SfLtqvqNoTa4j0jy/aoaHXYf8yHJ7cDdwF/RO0oOcBW9N0lU1deH19386/6NLAcOBB4FllTVU0l+kd7R868PtcEZcDpoVz8FjgAemlA/vFvWkgXjU0BVtSnJScBnkryQyb8C5OdWkg1TLQIOm89ehmwF8E7g/cAfVdX6JD9u7Zd/n+1VtQN4JskDVfUUQFX9OMl+8fvCENjVu4CbktwPPNzVRoEXA+dNudbPp0eTLK+q9QDdEcEbgTXAy4bb2rw7DDgZ+OGEeoD/O//tDEdV/RT4UJJPdz+30vbvkX9IclBVPQMcP15Mcgj7yZvGlv/jTaqqvpjkGHpfe72Y3j/yMeDWLvFbchawvb9QVduBs5L8t+G0NDSfB547Hoj9knxt/tsZrqoaA96S5HeAp4bdzxC9pqqehZ8F5LjnAGcPp6U94zkBSWqYnxOQpIYZApLUMENAkhpmCEhSwwwBSWrY/wfyhNlBky1sbwAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndef prepare_labels(y):\n    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, le = prepare_labels(train['diagnosis'])","execution_count":9,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/30) ,-4 ,128)\n    \n    return img\n\ndef preprocess_image_old(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/40) ,-4 ,128)\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GlassDataset(Dataset):\n    def __init__(self, df, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), y = None):\n        self.df = df\n        self.datatype = datatype\n        if self.datatype == 'train':\n            # self.image_files_list = [i for i in df['id_code'].values]\n            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.labels = y\n        else:\n            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n            self.labels = np.zeros((df.shape[0], 5))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files_list[idx]\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=img)\n        image = image['image']\n\n        img_name_short = self.image_files_list[idx].split('.')[0]\n\n        label = self.labels[idx]\n        if self.datatype == 'test':\n            return image, label, img_name\n        else:\n            return image, label","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 256\n\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    albumentations.JpegCompression(80),\n    albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.Normalize(),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomRotate90(),\n    AT.ToTensor()\n    ])\n\n\ndataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)\ntest_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\ntr, val = train_test_split(train.diagnosis, stratify=train.diagnosis, test_size=0.1)\ntrain_sampler = SubsetRandomSampler(list(tr.index))\nvalid_sampler = SubsetRandomSampler(list(val.index))\nbatch_size = 16\nnum_workers = 0\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/\")","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"['pytorch-pretrained-image-models',\n 'pretrained-pytorch-models',\n 'diabetic-retinopathy-resized',\n 'pretrained-models',\n 'aptos2019-blindness-detection',\n 'seresnext-aptos',\n 'seresnext50',\n 'aptos1',\n 'efficientnet-pytorch',\n 'vgg16',\n 'pytorch-pretrained-models']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv = pretrainedmodels.__dict__['se_resnext50_32x4d']( pretrained=None)\nmodel_conv.last_linear = nn.Linear(2048, 5)\nmodel_conv.load_state_dict(torch.load(\"../input/seresnext-aptos/model_1.pt\"))\nmodel_conv.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"SENet(\n  (layer0): Sequential(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu1): ReLU(inplace=True)\n    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n  )\n  (layer1): Sequential(\n    (0): SEResNeXtBottleneck(\n      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): SEResNeXtBottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (2): SEResNeXtBottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n  (layer2): Sequential(\n    (0): SEResNeXtBottleneck(\n      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): SEResNeXtBottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (2): SEResNeXtBottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (3): SEResNeXtBottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n  (layer3): Sequential(\n    (0): SEResNeXtBottleneck(\n      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (2): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (3): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (4): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (5): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n  (layer4): Sequential(\n    (0): SEResNeXtBottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): SEResNeXtBottleneck(\n      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n    (2): SEResNeXtBottleneck(\n      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (se_module): SEModule(\n        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n  (last_linear): Linear(in_features=2048, out_features=5, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs=15, attempt=1):\n    model_conv.to(device)\n    valid_loss_min = np.Inf\n    patience = 8\n    # current number of epochs, where validation loss didn't increase\n    p = 0\n    # whether training should be stopped\n    stop = False\n\n    # number of epochs to train the model\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = []\n        train_auc = []\n\n        for batch_i, (data, target) in enumerate(train_loader):\n\n            data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model_conv(data)\n            loss = criterion(output, target.float())\n            train_loss.append(loss.item())\n\n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # train_auc.append(roc_auc_score(a, b))\n            loss.backward()\n            optimizer.step()\n\n        model_conv.eval()\n        val_loss = []\n        val_auc = []\n        for batch_i, (data, target) in enumerate(valid_loader):\n            data, target = data.cuda(), target.cuda()\n            output = model_conv(data)\n\n            loss = criterion(output, target.float())\n\n            val_loss.append(loss.item()) \n            a = target.data.cpu().numpy()\n            b = output[:,-1].detach().cpu().numpy()\n            # val_auc.append(roc_auc_score(a, b))\n\n        # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n\n        valid_loss = np.mean(val_loss)\n        scheduler.step(valid_loss)\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model_conv.state_dict(), 'model_{}.pt'.format(attempt))\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n    return model_conv","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.SGD(model_conv.last_linear.parameters(), lr=0.001, momentum=0.99)\n#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\nscheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, )","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n                              optimizer = optimizer, n_epochs=20, attempt=1)","execution_count":24,"outputs":[{"output_type":"stream","text":"Thu Aug 22 14:18:22 2019 Epoch: 1\nEpoch 1, train loss: 0.2236, valid loss: 0.2502.\nValidation loss decreased (inf --> 0.250235).  Saving model ...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, test_loader):\n    sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n    model.eval()\n    for (data, target, name) in test_loader:\n        data = data.cuda()\n        output = model(data)\n        output = output.cpu().detach().numpy()\n        for i, (e, n) in enumerate(list(zip(output, name))):\n            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n    print( \"done\")\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_resnet101_1 = test_model(model_resnet101, test_loader)\ntest_resnet101_2 = test_model(model_resnet101, test_loader)\ntest_resnet101_3 = test_model(model_resnet101, test_loader)\ntest_resnet101_4 = test_model(model_resnet101, test_loader)\ntest_resnet101_5 = test_model(model_resnet101, test_loader)\ntest_resnet101_6 = test_model(model_resnet101, test_loader)\ntest_resnet101_7 = test_model(model_resnet101, test_loader)\ntest_resnet101_8 = test_model(model_resnet101, test_loader)\ntest_resnet101_9 = test_model(model_resnet101, test_loader)\ntest_resnet101_10 = test_model(model_resnet101, test_loader)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = (test_resnet101_1.diagnosis + test_resnet101_2.diagnosis + test_resnet101_3.diagnosis + test_resnet101_4.diagnosis + test_resnet101_5.diagnosis \n              + test_resnet101_6.diagnosis + test_resnet101_7.diagnosis  \n               + test_resnet101_8.diagnosis + test_resnet101_9.diagnosis + test_resnet101_10.diagnosis) / 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = [0.5, 1.5, 2.5, 3.5]\n\nfor i, pred in enumerate(test_preds):\n    if pred < coef[0]:\n        test_preds[i] = 0\n    elif pred >= coef[0] and pred < coef[1]:\n        test_preds[i] = 1\n    elif pred >= coef[1] and pred < coef[2]:\n        test_preds[i] = 2\n    elif pred >= coef[2] and pred < coef[3]:\n        test_preds[i] = 3\n    else:\n        test_preds[i] = 4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\nsample.diagnosis = test_preds.astype(int)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}