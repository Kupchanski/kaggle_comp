{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Important: in kernel-only competitions we can't use internet connections. So I use pretrained models from here: https://www.kaggle.com/bminixhofer/pytorch-pretrained-image-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"../input/pretrained-models/pretrained-models/pretrained-models.pytorch-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package_path = \"../input/efficientnet-pytorch/efficientnet-pytorch/EfficientNet-PyTorch-master\"\n",
    "sys.path.append(package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time \n",
    "import tqdm\n",
    "from PIL import Image\n",
    "train_on_gpu = True\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import copy\n",
    "import cv2\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "import random\n",
    "\n",
    "\n",
    "#import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "sample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "\n",
    "old_train = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels_cropped.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['diagnosis'].value_counts().plot(kind='bar');\n",
    "plt.title('Class counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a slight disbalance in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally I see little differences between images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "def prepare_labels(y):\n",
    "    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y, le = prepare_labels(train['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/30) ,-4 ,128)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_image_old(image_path, desired_size=224):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/40) ,-4 ,128)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlassDataset(Dataset):\n",
    "    def __init__(self, df, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), y = None):\n",
    "        self.df = df\n",
    "        self.datatype = datatype\n",
    "        if self.datatype == 'train':\n",
    "            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n",
    "            #self.image_files_list = [i for i in df['id_code'].values]\n",
    "            self.labels = y\n",
    "        else:\n",
    "            self.image_files_list = [f'../input/aptos2019-blindness-detection/{self.datatype}_images/{i}.png' for i in df['id_code'].values]\n",
    "            self.labels = np.zeros((df.shape[0], 5))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files_list[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = crop_image_from_gray(img)\n",
    "        image = self.transform(image=img)\n",
    "        image = image['image']\n",
    "\n",
    "        img_name_short = self.image_files_list[idx].split('.')[0]\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        if self.datatype == 'test':\n",
    "            return image, label, img_name\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = albumentations.Compose([\n",
    "    albumentations.Resize(256, 256),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.RandomBrightness(),\n",
    "    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n",
    "    albumentations.HueSaturationValue(),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "data_transforms_test = albumentations.Compose([\n",
    "    albumentations.Resize(256, 256),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.RandomRotate90(),\n",
    "    albumentations.Normalize(),   \n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "dataset = GlassDataset(df=train, datatype='train', transform=data_transforms, y=y)\n",
    "test_set = GlassDataset(df=test, datatype='test', transform=data_transforms_test)\n",
    "tr, val = train_test_split(train.diagnosis, stratify=train.diagnosis, test_size=0.15)\n",
    "train_sampler = SubsetRandomSampler(list(tr.index))\n",
    "valid_sampler = SubsetRandomSampler(list(val.index))\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../input/effenet4-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = EfficientNet.from_name('efficientnet-b4')\n",
    "in_features = model_conv._fc.in_features\n",
    "model_conv._fc = nn.Linear(in_features, 5)\n",
    "model_conv.load_state_dict(torch.load(\"../input/effenet4-2/model.pt\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, n_epochs=15, attempt=1):\n",
    "    model_conv.to(device)\n",
    "    valid_loss_min = np.Inf\n",
    "    patience = 5\n",
    "    # current number of epochs, where validation loss didn't increase\n",
    "    p = 0\n",
    "    # whether training should be stopped\n",
    "    stop = False\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "        train_loss = []\n",
    "        train_auc = []\n",
    "\n",
    "        for batch_i, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_conv(data)\n",
    "            loss = criterion(output, target.float())\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output[:,-1].detach().cpu().numpy()\n",
    "            # train_auc.append(roc_auc_score(a, b))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model_conv.eval()\n",
    "        val_loss = []\n",
    "        val_auc = []\n",
    "        for batch_i, (data, target) in enumerate(valid_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model_conv(data)\n",
    "\n",
    "            loss = criterion(output, target.float())\n",
    "\n",
    "            val_loss.append(loss.item()) \n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output[:,-1].detach().cpu().numpy()\n",
    "            # val_auc.append(roc_auc_score(a, b))\n",
    "\n",
    "        # print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid auc: {np.mean(val_auc):.4f}')\n",
    "        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n",
    "\n",
    "        valid_loss = np.mean(val_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model_conv.state_dict(), 'model_{}.pt'.format(attempt))\n",
    "            valid_loss_min = valid_loss\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_loss > valid_loss_min:\n",
    "            p += 1\n",
    "            print(f'{p} epochs of increasing val loss')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break        \n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.Adam(model_conv.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizer = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.99)\n",
    "#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, )\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  4 15:08:43 2019 Epoch: 1\n",
      "Epoch 1, train loss: 0.1066, valid loss: 0.0822.\n",
      "Validation loss decreased (inf --> 0.082180).  Saving model ...\n",
      "Wed Sep  4 15:22:40 2019 Epoch: 2\n",
      "Epoch 2, train loss: 0.0745, valid loss: 0.0733.\n",
      "Validation loss decreased (0.082180 --> 0.073322).  Saving model ...\n",
      "Wed Sep  4 15:34:07 2019 Epoch: 3\n",
      "Epoch 3, train loss: 0.0661, valid loss: 0.0665.\n",
      "Validation loss decreased (0.073322 --> 0.066536).  Saving model ...\n",
      "Wed Sep  4 15:45:20 2019 Epoch: 4\n",
      "Epoch 4, train loss: 0.0590, valid loss: 0.0629.\n",
      "Validation loss decreased (0.066536 --> 0.062881).  Saving model ...\n",
      "Wed Sep  4 15:57:03 2019 Epoch: 5\n",
      "Epoch 5, train loss: 0.0533, valid loss: 0.0622.\n",
      "Validation loss decreased (0.062881 --> 0.062214).  Saving model ...\n",
      "Wed Sep  4 16:09:20 2019 Epoch: 6\n",
      "Epoch 6, train loss: 0.0509, valid loss: 0.0615.\n",
      "Validation loss decreased (0.062214 --> 0.061508).  Saving model ...\n",
      "Wed Sep  4 16:20:56 2019 Epoch: 7\n",
      "Epoch 7, train loss: 0.0482, valid loss: 0.0582.\n",
      "Validation loss decreased (0.061508 --> 0.058202).  Saving model ...\n",
      "Wed Sep  4 16:32:20 2019 Epoch: 8\n",
      "Epoch 8, train loss: 0.0444, valid loss: 0.0595.\n",
      "1 epochs of increasing val loss\n",
      "Wed Sep  4 16:43:26 2019 Epoch: 9\n",
      "Epoch 9, train loss: 0.0406, valid loss: 0.0561.\n",
      "Validation loss decreased (0.058202 --> 0.056133).  Saving model ...\n",
      "Wed Sep  4 16:54:26 2019 Epoch: 10\n",
      "Epoch 10, train loss: 0.0408, valid loss: 0.0572.\n",
      "1 epochs of increasing val loss\n",
      "Wed Sep  4 17:05:18 2019 Epoch: 11\n",
      "Epoch 11, train loss: 0.0374, valid loss: 0.0585.\n",
      "2 epochs of increasing val loss\n",
      "Wed Sep  4 17:16:16 2019 Epoch: 12\n",
      "Epoch 12, train loss: 0.0348, valid loss: 0.0504.\n",
      "Validation loss decreased (0.056133 --> 0.050369).  Saving model ...\n",
      "Wed Sep  4 17:27:06 2019 Epoch: 13\n",
      "Epoch 13, train loss: 0.0321, valid loss: 0.0532.\n",
      "1 epochs of increasing val loss\n",
      "Wed Sep  4 17:37:48 2019 Epoch: 14\n",
      "Epoch 14, train loss: 0.0295, valid loss: 0.0563.\n",
      "2 epochs of increasing val loss\n",
      "Wed Sep  4 17:48:53 2019 Epoch: 15\n",
      "Epoch 15, train loss: 0.0295, valid loss: 0.0556.\n",
      "3 epochs of increasing val loss\n"
     ]
    }
   ],
   "source": [
    "model_resnet101 = train_model(model_conv, train_loader, valid_loader, criterion = criterion, \n",
    "                              optimizer = optimizer, n_epochs=15, attempt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def test_model(model, test_loader):\\n    sub = pd.read_csv(\\'../input/aptos2019-blindness-detection/sample_submission.csv\\')\\n    model.eval()\\n    for (data, target, name) in test_loader:\\n        data = data.cuda()\\n        output = model(data)\\n        output = output.cpu().detach().numpy()\\n        for i, (e, n) in enumerate(list(zip(output, name))):\\n            sub.loc[sub[\\'id_code\\'] == n.split(\\'/\\')[-1].split(\\'.\\')[0], \\'diagnosis\\'] = le.inverse_transform([np.argmax(e)])\\n    print( \"done\")\\n    return sub'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def test_model(model, test_loader):\n",
    "    sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "    model.eval()\n",
    "    for (data, target, name) in test_loader:\n",
    "        data = data.cuda()\n",
    "        output = model(data)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        for i, (e, n) in enumerate(list(zip(output, name))):\n",
    "            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n",
    "    print( \"done\")\n",
    "    return sub\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_resnet101_1 = test_model(model_resnet101, test_loader)\\ntest_resnet101_2 = test_model(model_resnet101, test_loader)\\ntest_resnet101_3 = test_model(model_resnet101, test_loader)\\ntest_resnet101_4 = test_model(model_resnet101, test_loader)\\ntest_resnet101_5 = test_model(model_resnet101, test_loader)\\ntest_resnet101_6 = test_model(model_resnet101, test_loader)\\ntest_resnet101_7 = test_model(model_resnet101, test_loader)\\ntest_resnet101_8 = test_model(model_resnet101, test_loader)\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_resnet101_1 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_2 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_3 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_4 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_5 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_6 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_7 = test_model(model_resnet101, test_loader)\n",
    "test_resnet101_8 = test_model(model_resnet101, test_loader)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_preds = (test_resnet101_1.diagnosis + test_resnet101_2.diagnosis + test_resnet101_3.diagnosis + test_resnet101_4.diagnosis + test_resnet101_5.diagnosis \\n              + test_resnet101_6.diagnosis + test_resnet101_7.diagnosis  \\n               + test_resnet101_8.diagnosis ) / 8'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_preds = (test_resnet101_1.diagnosis + test_resnet101_2.diagnosis + test_resnet101_3.diagnosis + test_resnet101_4.diagnosis + test_resnet101_5.diagnosis \n",
    "              + test_resnet101_6.diagnosis + test_resnet101_7.diagnosis  \n",
    "               + test_resnet101_8.diagnosis ) / 8\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coef = [0.5, 1.5, 2.5, 3.5]\\n\\nfor i, pred in enumerate(test_preds):\\n    if pred < coef[0]:\\n        test_preds[i] = 0\\n    elif pred >= coef[0] and pred < coef[1]:\\n        test_preds[i] = 1\\n    elif pred >= coef[1] and pred < coef[2]:\\n        test_preds[i] = 2\\n    elif pred >= coef[2] and pred < coef[3]:\\n        test_preds[i] = 3\\n    else:\\n        test_preds[i] = 4\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\\nsample.diagnosis = test_preds.astype(int)\\nsample.to_csv(\"submission.csv\", index=False)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = test_preds.astype(int)\n",
    "sample.to_csv(\"submission.csv\", index=False)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
