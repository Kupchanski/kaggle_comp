{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom pprint import pprint\nfrom tqdm import tqdm","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Preprocessing\n\nThis preprocessing method is more careful with RAM usage, which avoids crashing the kernel when you switch from CPU to GPU."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\ngc.collect()\n\nX_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":2,"outputs":[{"output_type":"stream","text":"(590540, 433)\n(506691, 432)\nCPU times: user 1min 34s, sys: 13.8 s, total: 1min 48s\nWall time: 1min 48s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# RAM optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":4,"outputs":[{"output_type":"stream","text":"Memory usage of dataframe is 1970.87 MB\nMemory usage after optimization is: 547.14 MB\nDecreased by 72.2%\nMemory usage of dataframe is 1673.87 MB\nMemory usage after optimization is: 460.02 MB\nDecreased by 72.5%\nCPU times: user 1min 57s, sys: 5min 33s, total: 7min 30s\nWall time: 7min 31s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Training\n\nDAYS OF RESEARCH BROUGHT ME TO THE CONCLUSION THAT I SHOULD SIMPLY SPECIFY `tree_method='gpu_hist'` IN ORDER TO ACTIVATE GPU (okay jk, took me an hour to figure out, but I wish XGBoost documentation was more clear about that)."},{"metadata":{},"cell_type":"markdown","source":"# GridSearch Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class XGBGridSearch:\n    \"\"\"\n    Source:\n    https://www.kaggle.com/xhlulu/ieee-fraud-efficient-grid-search-with-xgboost\n    \"\"\"\n    def __init__(self, param_grid, cv=3, verbose=0, \n                 shuffle=False, random_state=2019):\n        self.param_grid = param_grid\n        self.cv = cv\n        self.random_state = random_state\n        self.verbose = verbose\n        self.shuffle = shuffle\n        \n        self.scores = []\n    \n    def fit(self, X, y):\n        self._expand_params()\n        self._split_data(X, y)\n            \n        for params in tqdm(self.param_list, disable=not self.verbose):\n            score = self._cv_score(X, y, params)\n            self.scores.append(score)\n        \n        self._compute_best()\n\n    def _cv_score(self, X, y, params):\n        \"\"\"\n        Perform KFold CV on a single set of parameters\n        \"\"\"\n        scores = []\n        \n        for train_idx, val_idx in self.splits:\n            clf = xgb.XGBClassifier(**params)\n\n            X_train, X_val = X.iloc[train_idx, :], X.iloc[val_idx, :]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            clf.fit(X_train, y_train)\n            \n            y_val_pred = clf.predict_proba(X_val)[:, 1]\n            \n            score = roc_auc_score(y_val, y_val_pred)\n            scores.append(score)\n            \n            gc.collect()\n        \n        avg_score = sum(scores) / len(scores)\n        return avg_score\n            \n    def _split_data(self, X, y):\n        kf = KFold(n_splits=self.cv, \n                   shuffle=self.shuffle, \n                   random_state=self.random_state)\n        self.splits = list(kf.split(X, y))\n            \n    def _compute_best(self):\n        \"\"\"\n        Compute best params and its corresponding score\n        \"\"\"\n        idx_best = np.argmax(self.scores)\n        self.best_score_ = self.scores[idx_best]\n        self.best_params_ = self.param_list[idx_best]\n\n    def _expand_params(self):\n        \"\"\"\n        This method expands a dictionary of lists into\n        a list of dictionaries (each dictionary is a single\n        valid params that can be input to XGBoost)\n        \"\"\"\n        keys, values = zip(*self.param_grid.items())\n        self.param_list = [\n            dict(zip(keys, v)) \n            for v in itertools.product(*values)\n        ]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [500],\n    'missing': [-999],\n    'random_state': [2019],\n    'n_jobs': [1],\n    'tree_method': ['gpu_hist'],\n    'max_depth': [9],\n    'learning_rate': [0.05],\n    'subsample': [0.9],\n    'colsample_bytree': [0.9],\n    'reg_alpha': [0],\n    'reg_lambda': [1]\n}","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = XGBGridSearch(param_grid, cv=4, verbose=1)\n%time grid.fit(X_train, y_train)\n\nprint(\"Best Score:\", grid.best_score_)\nprint(\"Best Params:\", grid.best_params_)","execution_count":7,"outputs":[{"output_type":"stream","text":"100%|██████████| 1/1 [02:28<00:00, 148.17s/it]","name":"stderr"},{"output_type":"stream","text":"CPU times: user 55.3 s, sys: 1min 32s, total: 2min 27s\nWall time: 2min 28s\nBest Score: 0.91864115909341\nBest Params: {'n_estimators': 500, 'missing': -999, 'random_state': 2019, 'n_jobs': 1, 'tree_method': 'gpu_hist', 'max_depth': 9, 'learning_rate': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 1}\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(**grid.best_params_)\nclf.fit(X_train, y_train)\n","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.9, gamma=0,\n              learning_rate=0.05, max_delta_step=0, max_depth=9,\n              min_child_weight=1, missing=-999, n_estimators=500, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=2019,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=0.9, tree_method='gpu_hist', verbosity=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('IEEExgboost.csv')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}